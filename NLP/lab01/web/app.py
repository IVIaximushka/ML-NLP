import streamlit as st
import pandas as pd
import json
import plotly.express as px
from utils.tokenization import apply_tokenization_pipeline
from utils.metrics import calculate_basic_metrics, calculate_oov_rate, calculate_processing_speed
from utils.visualization import (create_token_length_distribution, create_frequency_plot,
                               create_top_tokens_chart, create_metrics_comparison, create_wordcloud)
import tempfile
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("üìä –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞")
st.markdown("""
–≠—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.
–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞.
""")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

# –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data_source = st.sidebar.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö:", 
                              ["–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö", "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª"])

texts = []
if data_source == "–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö":
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    sample_texts = [
        "–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞ –ö–∞—Å—ã–º-–ñ–æ–º–∞—Ä—Ç –¢–æ–∫–∞–µ–≤ –ø—Ä–æ–≤–µ–ª –≤—Å—Ç—Ä–µ—á—É —Å –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –£–∫—Ä–∞–∏–Ω—ã –í–ª–∞–¥–∏–º–∏—Ä–æ–º –ó–µ–ª–µ–Ω—Å–∫–∏–º.",
        "–ì–ª–∞–≤—ã –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤ –æ–±—Å—É–¥–∏–ª–∏ –≤–æ–ø—Ä–æ—Å—ã —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–∞–Ω–∞–º–∏.",
        "–í—Å—Ç—Ä–µ—á–∞ —Å–æ—Å—Ç–æ—è–ª–∞—Å—å –≤ —Ä–∞–º–∫–∞—Ö –∑–∞—Å–µ–¥–∞–Ω–∏—è –ì–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–π –ê—Å—Å–∞–º–±–ª–µ–∏ –û–û–ù –≤ –ù—å—é-–ô–æ—Ä–∫–µ.",
        "–°—Ç–æ—Ä–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å –∞–∫—Ç–∏–≤–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—Ä–≥–æ–≤–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è."
    ]
    texts = sample_texts
    st.sidebar.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")

else:
    uploaded_file = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ JSONL —Ñ–∞–π–ª", type=['jsonl', 'json'])
    if uploaded_file is not None:
        try:
            # –ß—Ç–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            lines = uploaded_file.getvalue().decode('utf-8').splitlines()
            for line in lines:
                data = json.loads(line)
                text = data.get('header', '') + ' ' + data.get('text', '')
                texts.append(text)
            st.sidebar.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        except Exception as e:
            st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
st.sidebar.subheader("–ú–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")

col1, col2 = st.sidebar.columns(2)

with col1:
    tokenizer_method = st.selectbox(
        "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:",
        ["razdel", "spacy", "nltk", "naive", "regex"],
        help="–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
    )

with col2:
    normalizer_method = st.selectbox(
        "–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:",
        ["none", "pymorphy2", "spacy", "porter", "snowball"],
        help="–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤"
    )

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
max_texts = st.sidebar.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:", 
                             min_value=10, max_value=1000, value=100 if len(texts) > 100 else len(texts))

# –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
analyze_button = st.sidebar.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑", type="primary")

# –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
if analyze_button and texts:
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤
    analysis_texts = texts[:max_texts]
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    status_text.text("–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
    all_tokens = []
    
    for i, text in enumerate(analysis_texts):
        tokens = apply_tokenization_pipeline(text, tokenizer_method, normalizer_method)
        all_tokens.append(tokens)
        progress_bar.progress((i + 1) / len(analysis_texts))
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    status_text.text("–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫...")
    metrics = calculate_basic_metrics(all_tokens)
    
    # –†–∞—Å—á–µ—Ç OOV rate (–µ—Å–ª–∏ –µ—Å—Ç—å –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
    base_tokens = []
    for text in analysis_texts:
        base_tokens.append(apply_tokenization_pipeline(text, "razdel", "none"))
    
    oov_rate = calculate_oov_rate(base_tokens, all_tokens)
    
    # –†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏
    speed = calculate_processing_speed(analysis_texts[:10], 
                                     lambda x: apply_tokenization_pipeline(x, tokenizer_method, normalizer_method))
    
    status_text.text("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    st.header("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
    
    # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤", f"{metrics.get('total_tokens', 0):,}")
    
    with col2:
        st.metric("–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è", f"{metrics.get('vocabulary_size', 0):,}")
    
    with col3:
        st.metric("–î–æ–ª—è OOV", f"{oov_rate:.2%}")
    
    with col4:
        st.metric("–°–∫–æ—Ä–æ—Å—Ç—å (—Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫)", f"{speed:.1f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    tab1, tab2, tab3, tab4 = st.tabs(["üìè –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω", "üìä –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å", "üèÜ –¢–æ–ø —Ç–æ–∫–µ–Ω—ã", "‚òÅÔ∏è –û–±–ª–∞–∫–æ —Å–ª–æ–≤"])
    
    with tab1:
        if metrics.get('token_lengths'):
            fig_length = create_token_length_distribution(metrics['token_lengths'])
            st.plotly_chart(fig_length, use_container_width=True)
    
    with tab2:
        if metrics.get('frequencies'):
            fig_freq = create_frequency_plot(metrics['frequencies'])
            st.plotly_chart(fig_freq, use_container_width=True)
    
    with tab3:
        if metrics.get('most_common_tokens'):
            fig_top = create_top_tokens_chart(metrics['most_common_tokens'])
            st.plotly_chart(fig_top, use_container_width=True)
    
    with tab4:
        wordcloud_img = create_wordcloud(all_tokens)
        if wordcloud_img:
            st.image(wordcloud_img, use_column_width=True)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    
    metrics_df = pd.DataFrame([{
        '–ú–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏': tokenizer_method,
        '–ú–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏': normalizer_method,
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤': len(analysis_texts),
        '–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤': metrics.get('total_tokens', 0),
        '–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã': metrics.get('unique_tokens', 0),
        '–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞': f"{metrics.get('avg_token_length', 0):.2f}",
        '–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ': f"{metrics.get('lexical_diversity', 0):.4f}",
        '–î–æ–ª—è OOV': f"{oov_rate:.2%}",
        '–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏': f"{speed:.1f} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫"
    }])
    
    st.dataframe(metrics_df, use_container_width=True)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ)
    st.subheader("üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤")
    
    compare_methods = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:",
        ["razdel+none", "razdel+pymorphy2", "spacy+spacy", "nltk+porter"],
        default=["razdel+none", "razdel+pymorphy2"]
    )
    
    if st.button("–°—Ä–∞–≤–Ω–∏—Ç—å –º–µ—Ç–æ–¥—ã") and compare_methods:
        comparison_metrics = {}
        
        for method in compare_methods:
            tokenizer, normalizer = method.split('+')
            comp_tokens = []
            
            for text in analysis_texts[:50]:  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                tokens = apply_tokenization_pipeline(text, tokenizer, normalizer)
                comp_tokens.append(tokens)
            
            comp_metrics = calculate_basic_metrics(comp_tokens)
            comparison_metrics[method] = comp_metrics
        
        fig_compare = create_metrics_comparison(comparison_metrics)
        st.plotly_chart(fig_compare, use_container_width=True)
    
    # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –≤ CSV
        csv = metrics_df.to_csv(index=False, encoding='utf-8')
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (CSV)",
            data=csv,
            file_name=f"tokenization_metrics_{tokenizer_method}_{normalizer_method}.csv",
            mime="text/csv"
        )
    
    with col2:
        # –≠–∫—Å–ø–æ—Ä—Ç —Ç–æ–∫–µ–Ω–æ–≤
        tokens_export = "\n".join([" ".join(tokens) for tokens in all_tokens])
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å —Ç–æ–∫–µ–Ω—ã (TXT)",
            data=tokens_export,
            file_name=f"tokens_{tokenizer_method}_{normalizer_method}.txt",
            mime="text/plain"
        )
    
    status_text.text("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")

elif not texts:
    st.info("üëà –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞")

else:
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
    st.markdown("""
    ## üéØ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    
    ### üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
    - **–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤** - –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –¥–ª–∏–Ω –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    - **–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤** - –∑–∞–∫–æ–Ω –¶–∏–ø—Ñ–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç
    - **–¢–æ–ø-20 —Ç–æ–∫–µ–Ω–æ–≤** - —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    - **–û–±–ª–∞–∫–æ —Å–ª–æ–≤** - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    
    ### ‚öôÔ∏è –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–µ—Ç–æ–¥—ã:
    **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:**
    - `razdel` - –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    - `spacy` - –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ NLP
    - `nltk` - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
    - `naive` - –Ω–∞–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–µ–ª–∞–º
    - `regex` - —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
    
    **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:**
    - `none` - –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    - `pymorphy2` - –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
    - `spacy` - –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ spaCy
    - `porter` - —Å—Ç–µ–º–º–∏–Ω–≥ –ü–æ—Ä—Ç–µ—Ä–∞
    - `snowball` - —Å—Ç–µ–º–º–∏–Ω–≥ –°–Ω–æ—É–±–æ–ª–ª
    
    ### üöÄ –ß—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å:
    1. –í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
    2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    3. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É "–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑"
    4. –ò–∑—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–∞—Ö
    """)

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown("*–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ v1.0 | –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–ª—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç–æ–¥–æ–≤ NLP*")
